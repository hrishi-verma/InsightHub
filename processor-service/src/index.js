const { Kafka } = require('kafkajs');
const { Pool } = require('pg');
require('dotenv').config();

const kafka = new Kafka({
  clientId: 'insighthub-processor',
  brokers: [process.env.KAFKA_BROKER || 'localhost:9092']
});

const consumer = kafka.consumer({ groupId: 'log-processors' });

const pool = new Pool({
  host: process.env.DB_HOST || 'localhost',
  port: process.env.DB_PORT || 5432,
  database: process.env.DB_NAME || 'insighthub',
  user: process.env.DB_USER || 'postgres',
  password: process.env.DB_PASSWORD
});

// Step 1: Normalize log structure
function normalizeLog(rawLog) {
  return {
    service: rawLog.service || 'unknown',
    level: (rawLog.level || 'INFO').toUpperCase(),
    message: rawLog.message || '',
    latency_ms: parseInt(rawLog.latency_ms) || null,
    userId: rawLog.userId || null,
    metadata: rawLog.metadata || {}
  };
}

// Step 2: Add timestamps and IDs (ID is auto-generated by DB)
function addTimestamp(log) {
  return {
    ...log,
    created_at: log.timestamp || new Date().toISOString()
  };
}

// Step 3: Convert to time-series compatible structure
function convertToTimeSeries(log) {
  return {
    ...log,
    // Ensure timestamp is in ISO format for TimescaleDB
    created_at: new Date(log.created_at).toISOString(),
    // Add time bucket for aggregations (optional)
    time_bucket: new Date(log.created_at).toISOString().slice(0, 16) + ':00.000Z'
  };
}

// Step 4: Call ML engine for anomaly scoring
async function getAnomalyScore(log) {
  try {
    const mlEngineUrl = process.env.ML_ENGINE_URL || 'http://ml-engine:5000';
    
    // Try ML engine API first
    try {
      const response = await fetch(`${mlEngineUrl}/score`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          service: log.service,
          level: log.level,
          message: log.message,
          latency_ms: log.latency_ms
        }),
        timeout: 2000
      });
      
      if (response.ok) {
        const result = await response.json();
        console.log(`  ML Engine: ${result.methods ? 'Combined' : 'Single'} method, score: ${result.anomaly_score.toFixed(2)}`);
        return result;
      }
    } catch (mlError) {
      console.log('  ML Engine unavailable, using fallback heuristics');
    }
    
    // Fallback to heuristic-based scoring
    let score = 0;
    let isAnomaly = false;
    
    // High latency detection
    if (log.latency_ms && log.latency_ms > 1000) {
      score += 0.5;
    }
    
    // Error level detection
    if (log.level === 'ERROR') {
      score += 0.3;
    }
    
    // Critical latency
    if (log.latency_ms && log.latency_ms > 2000) {
      score += 0.3;
    }
    
    // Mark as anomaly if score exceeds threshold
    isAnomaly = score >= 0.7;
    
    return {
      anomaly_score: Math.min(score, 1.0),
      is_anomaly: isAnomaly,
      method: 'heuristic_fallback'
    };
  } catch (error) {
    console.error('Error getting anomaly score:', error);
    return { anomaly_score: 0, is_anomaly: false, method: 'error_fallback' };
  }
}

// Step 5: Store in database
async function storeLog(log, anomalyData) {
  const query = `
    INSERT INTO logs (
      service, 
      level, 
      message, 
      latency_ms, 
      anomaly_score, 
      is_anomaly, 
      created_at
    )
    VALUES ($1, $2, $3, $4, $5, $6, $7)
    RETURNING log_id
  `;
  
  const values = [
    log.service,
    log.level,
    log.message,
    log.latency_ms,
    anomalyData.anomaly_score,
    anomalyData.is_anomaly,
    log.created_at
  ];
  
  const result = await pool.query(query, values);
  return result.rows[0].log_id;
}

// Main processing pipeline
async function processLog(rawLog) {
  try {
    // Step 1: Normalize
    const normalized = normalizeLog(rawLog);
    
    // Step 2: Add timestamp
    const timestamped = addTimestamp(normalized);
    
    // Step 3: Convert to time-series format
    const timeSeries = convertToTimeSeries(timestamped);
    
    // Step 4: Get anomaly score from ML engine
    const anomalyData = await getAnomalyScore(timeSeries);
    
    // Step 5: Store in database
    const logId = await storeLog(timeSeries, anomalyData);
    
    // Log processing result
    const anomalyFlag = anomalyData.is_anomaly ? 'ðŸš¨ ANOMALY' : 'âœ“';
    console.log(
      `${anomalyFlag} Processed log #${logId}:`,
      `${timeSeries.service} [${timeSeries.level}]`,
      anomalyData.is_anomaly ? `(score: ${anomalyData.anomaly_score.toFixed(2)})` : ''
    );
    
    return logId;
  } catch (error) {
    console.error('Error in processing pipeline:', error);
    throw error;
  }
}

// Kafka consumer setup
async function run() {
  try {
    // Connect to Kafka
    await consumer.connect();
    console.log('âœ“ Connected to Kafka');
    
    // Subscribe to logs topic
    await consumer.subscribe({ topic: 'logs', fromBeginning: false });
    console.log('âœ“ Subscribed to "logs" topic');
    
    // Test database connection
    await pool.query('SELECT NOW()');
    console.log('âœ“ Connected to PostgreSQL');
    
    // Start consuming messages
    await consumer.run({
      eachMessage: async ({ topic, partition, message }) => {
        try {
          const rawLog = JSON.parse(message.value.toString());
          await processLog(rawLog);
        } catch (error) {
          console.error('Error processing message:', error);
          // Continue processing other messages
        }
      },
    });
    
    console.log('ðŸš€ Processor service started and ready');
  } catch (error) {
    console.error('Failed to start processor:', error);
    process.exit(1);
  }
}

// Graceful shutdown
process.on('SIGTERM', async () => {
  console.log('Shutting down processor...');
  await consumer.disconnect();
  await pool.end();
  process.exit(0);
});

// Start the processor
run().catch(console.error);
